XGB
算法原理
	XGB基于特征生成多颗回归树，每颗回归树学习相应的残差，残差之和即为样本的预测值。

损失函数
	回归问题：
		MSE（）

	分类问题：
		对数损失函数

分裂结点算法
	与CART类似，设定阈值，当目标函数的增益大于阈值时，进行结点分裂

正则化
	采用L2正则化

对缺失值处理
	为缺失值指定默认方向，而在预测中出现缺失值，划分到右子树。
优缺点
	优点：
		1.能很好地处理缺失值，学习分裂方向
		2.支持线性分类器，L1正则化（逻辑回归），L2正则化（线性回归）
		3. 引入阈值（预剪枝）和正则化，减少模型复杂度，防止过拟合。
	缺点：
		对结点特征预排序，算法复杂度高，需采用贪心策略

应用场景
	

sklearn参数